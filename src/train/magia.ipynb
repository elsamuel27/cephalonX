{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He plasmado todos los datos dentro de archivos CSV, que están en la carpeta [\"data\"](data/), por lo que ahora para cargarlos sólo hay que crear un DataFrame de Pandas vacío y juntar en él los datos de todos los archivos. Un bucle \"for\" nos hace el trabajo independientemente de cuántos archivos estén (Siempre y cuando todos tengan la misma estructura claro, pero estoy cuidando eso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()\n",
    "\n",
    "for archivo in os.listdir(\"./data\"):\n",
    "    if archivo.endswith(\".csv\"):\n",
    "        dataset = pd.concat([dataset, pd.read_csv(os.path.join(\"./data\", archivo), delimiter=\",\", quotechar='\"', header=None)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método para almacenar los datos funciona, pero ello no implica que un modelo pueda entenderlos. Deben ser tokenizados, para ello Keras lleva un tokenizador y nos sirve con los datos así como los he guardado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En éste caso es tan fácil como, inicializar el tokenizador y usar su método `.fit_on_texts()` para ajustar su vocabulario y demás al de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(dataset[0] + dataset[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al estar éste tokenizador ajustado a los datos de entrenamiento, y teniendo en cuenta que se va a usar para cargar todos los datos que el modelo vaya a leer o escribir, es conveniente guardarlo para futuras ejecuciones, de la misma forma que se guarda el modelo. La siguiente celda se encarga de éso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_config.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(tokenizer.get_config()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar el tokenizador exportado se usaría éste código: \n",
    "```python\n",
    "with open('tokenizer_config.txt', 'r', encoding='utf-8') as f:\n",
    "    tokenizer_config_loaded = f.read()\n",
    "    tokenizer_config_dict = eval(tokenizer_config_loaded)\n",
    "    tokenizer_loaded = Tokenizer.from_config(tokenizer_config_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = tokenizer.texts_to_sequences(dataset[0])\n",
    "respuestas = tokenizer.texts_to_sequences(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar padding a las secuencias\n",
    "max_len = max([len(seq) for seq in preguntas + respuestas])\n",
    "preguntas_padded = tf.keras.preprocessing.sequence.pad_sequences(preguntas, maxlen=max_len)\n",
    "respuestas_padded = tf.keras.preprocessing.sequence.pad_sequences(respuestas, maxlen=max_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenar aleatoriamente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una buena práctica otorgarle al modelo los datos ordenados de manera aleatoria, y de hecho por cómo estamos cargando los datos tenemos una ventaja adicional:\n",
    "- El modelo no aprende patrones raros en función del orden en el que reciba datos\n",
    "- Al momento de reservar una parte para validación, nos aseguramos de que tanto en entrenamiento como en validación hay datos de todas las categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el índice de división\n",
    "split_idx = int(0.8 * len(preguntas_padded))\n",
    "\n",
    "# Barajar los índices\n",
    "indices = np.arange(len(preguntas_padded))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reordenar los datos según los índices barajados\n",
    "preguntas_padded_shuffled = preguntas_padded[indices]\n",
    "respuestas_padded_shuffled = respuestas_padded[indices]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val = np.split(preguntas_padded_shuffled, [split_idx])\n",
    "y_train, y_val = np.split(respuestas_padded_shuffled, [split_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado de los datos tiene buena pinta, toca ir preparando el modelo (Y si no siempre se puede agregar más código arriba)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras tiene un objeto que se coloca en los callbacks de `.fit()` y sirve para ir guardando el modelo a medida que entrena, lo que nos permite cortar el entrenamiento y continuarlo después o directamente llevar uno de los guardados a prueba manual, y tal vez producción\n",
    "\n",
    "Asimismo, los argumentos del checkpointer sirven para elegir entre hacer varias cosas, por ejemplo...\n",
    "- Ir guardando cada copia del modelo poniéndole una variable al nombre\n",
    "- Que el nombre sea invariable y una copia se sobreescriba sobre la anterior, teniendo siempre 1\n",
    "- Guardar una copia tras cada etapa\n",
    "- Guardar una copia tras cada etapa pero sólo si cumple algún requisito, que puede ser haber bajado o subido alguna variable\n",
    "\n",
    "En éste caso particular la configuración será la siguiente: \n",
    "- Sólo se guarda copia si el modelo ha bajado su pérdida de validación, pues es la forma más rápida y sencilla de ver si está mejorando\n",
    "- Cada copia sobreescribe a la anterior. En teoría, si se está guardando una copia, significa que es como la anterior pero mejor, por lo que para qué conservar la antigua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoints/model.tf\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=min,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Éste objeto de Keras se encarga de ir guardando el modelo en mitad del entrenamiento. Al no tener ninguna variable el nombre (aunque se le podrían poner), en la ruta sólo va a haber un modelo siempre, y gracias al resto de parámetros, siempre será el que haya sacado la pérdida de validación (val_loss) más baja"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo el código ha sido escrito por [Phind](https://www.phind.com/) así que ni lo termino de entender ni sé si funcionará, pero a darle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "# Dimensiones de los embeddings y de las unidades LSTM\n",
    "embedding_dim = 128\n",
    "lstm_units = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_len,))\n",
    "encoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_len,))\n",
    "decoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Modelo\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
